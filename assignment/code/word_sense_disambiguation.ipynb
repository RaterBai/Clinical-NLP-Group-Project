{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T03:06:14.147603Z",
     "start_time": "2021-03-17T03:06:13.700175Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T03:06:14.890879Z",
     "start_time": "2021-03-17T03:06:14.718110Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(f\"{path}AnonymizedClinicalAbbreviationsAndAcronymsDataSet.txt\", \n",
    "                   encoding='cp1252', \n",
    "                   sep=\"|\", \n",
    "                   header=None,\n",
    "                   na_filter=False)\n",
    "data.columns = [\"abbrev\", \"sense\", \"represntaion\", \"start_pos\", \"end_pos\", \"section_info\", \"sample\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abbrev</th>\n",
       "      <th>sense</th>\n",
       "      <th>represntaion</th>\n",
       "      <th>start_pos</th>\n",
       "      <th>end_pos</th>\n",
       "      <th>section_info</th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AB</td>\n",
       "      <td>abortion</td>\n",
       "      <td>AB.</td>\n",
       "      <td>231</td>\n",
       "      <td>233</td>\n",
       "      <td></td>\n",
       "      <td>_%#NAME#%_ _%#NAME#%_ is a 29-year-old gravida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AB</td>\n",
       "      <td>abortion</td>\n",
       "      <td>AB.</td>\n",
       "      <td>249</td>\n",
       "      <td>251</td>\n",
       "      <td></td>\n",
       "      <td>She is now bleeding quite heavily. Ultrasound ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AB</td>\n",
       "      <td>abortion</td>\n",
       "      <td>AB</td>\n",
       "      <td>223</td>\n",
       "      <td>224</td>\n",
       "      <td>PAST OB HISTORY</td>\n",
       "      <td>ALLERGIES: Heparin and Imitrex. PAST OB HISTOR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AB</td>\n",
       "      <td>abortion</td>\n",
       "      <td>AB.</td>\n",
       "      <td>194</td>\n",
       "      <td>196</td>\n",
       "      <td>HISTORY OF THE PRESENT ILLNESS</td>\n",
       "      <td>She had a pelvic ultrasound at Park Nicollet o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AB</td>\n",
       "      <td>abortion</td>\n",
       "      <td>AB</td>\n",
       "      <td>114</td>\n",
       "      <td>115</td>\n",
       "      <td>PAST OB-GYN HISTORY</td>\n",
       "      <td>On _%#MMDD2007#%_, normal anatomy with anterio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  abbrev     sense represntaion start_pos end_pos  \\\n",
       "0     AB  abortion          AB.       231     233   \n",
       "1     AB  abortion          AB.       249     251   \n",
       "2     AB  abortion           AB       223     224   \n",
       "3     AB  abortion          AB.       194     196   \n",
       "4     AB  abortion           AB       114     115   \n",
       "\n",
       "                     section_info  \\\n",
       "0                                   \n",
       "1                                   \n",
       "2                 PAST OB HISTORY   \n",
       "3  HISTORY OF THE PRESENT ILLNESS   \n",
       "4             PAST OB-GYN HISTORY   \n",
       "\n",
       "                                              sample  \n",
       "0  _%#NAME#%_ _%#NAME#%_ is a 29-year-old gravida...  \n",
       "1  She is now bleeding quite heavily. Ultrasound ...  \n",
       "2  ALLERGIES: Heparin and Imitrex. PAST OB HISTOR...  \n",
       "3  She had a pelvic ultrasound at Park Nicollet o...  \n",
       "4  On _%#MMDD2007#%_, normal anatomy with anterio...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T03:06:17.980050Z",
     "start_time": "2021-03-17T03:06:17.959683Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_abbrev = np.unique(data.abbrev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T03:06:18.804030Z",
     "start_time": "2021-03-17T03:06:18.801346Z"
    }
   },
   "outputs": [],
   "source": [
    "empty_dict = dict.fromkeys(['abbrev','number_sense'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T03:06:20.772320Z",
     "start_time": "2021-03-17T03:06:19.828239Z"
    }
   },
   "outputs": [],
   "source": [
    "abbrev_freq = pd.DataFrame(columns=[\"abbrev\", \"number_sense\", \"sense\", \"freq\", \"percentage\"])\n",
    "#abbrev_freq_dict = dict.fromkeys(['abbrev','number_sense'])\n",
    "\n",
    "count_list = []\n",
    "for abbrev in unique_abbrev:\n",
    "    piece = data.loc[data.abbrev == abbrev]\n",
    "    senses = np.unique(piece.sense)\n",
    "    count = len(np.unique(piece.sense))\n",
    "    for sense in senses:\n",
    "        count_sense = piece.loc[piece.sense == sense].shape[0]\n",
    "        percentage = count_sense/piece.shape[0]\n",
    "        new = pd.DataFrame({\"abbrev\" : [abbrev],\n",
    "                            \"number_sense\" : [count],\n",
    "                            \"sense\" : [sense],\n",
    "                            \"freq\" : [count_sense],\n",
    "                            \"percentage\" : [percentage]})\n",
    "        \n",
    "        abbrev_freq = pd.concat([abbrev_freq, new])\n",
    "    #count_list.append(count)\n",
    "    \n",
    "#abbrev_freq = pd.DataFrame({\"abbrev\" : unique_abbrev, \"number_sense\" : count_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T03:06:21.372583Z",
     "start_time": "2021-03-17T03:06:21.368161Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AB', 'AC', 'ALD', 'AMA', 'ASA', 'AV', 'AVR', 'BAL', 'BK', 'BM',\n",
       "       'BMP', 'C&S', 'C3', 'C4', 'CA', 'CDI', 'CEA', 'CR', 'CTA', 'CVA',\n",
       "       'CVP', 'CVS', 'DC', 'DIP', 'DM', 'DT', 'EC', 'ER', 'ES', 'ET',\n",
       "       'FISH', 'FSH', 'GT', 'IA', 'IB', 'IM', 'IR', 'IT', 'ITP', 'IVF',\n",
       "       'LA', 'LE', 'MOM', 'MP', 'MR', 'MS', 'MSSA', 'NA', 'NAD', 'NP',\n",
       "       'OP', 'OR', 'OTC', 'PA', 'PAC', 'PCP', 'PD', 'PDA', 'PE', 'PM',\n",
       "       'PR', 'PT', 'RA', 'RT', 'SA', 'SBP', 'SMA', 'SS', 'T1', 'T2', 'T3',\n",
       "       'T4', 'US', 'VAD', 'VBG'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(abbrev_freq.abbrev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T04:20:43.534721Z",
     "start_time": "2021-03-17T04:20:43.524890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abbrev</th>\n",
       "      <th>number_sense</th>\n",
       "      <th>sense</th>\n",
       "      <th>freq</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMA</td>\n",
       "      <td>3</td>\n",
       "      <td>advanced maternal age</td>\n",
       "      <td>31</td>\n",
       "      <td>0.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMA</td>\n",
       "      <td>3</td>\n",
       "      <td>against medical advice</td>\n",
       "      <td>444</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMA</td>\n",
       "      <td>3</td>\n",
       "      <td>antimitochondrial antibody</td>\n",
       "      <td>25</td>\n",
       "      <td>0.050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  abbrev number_sense                       sense freq  percentage\n",
       "0    AMA            3       advanced maternal age   31       0.062\n",
       "0    AMA            3      against medical advice  444       0.888\n",
       "0    AMA            3  antimitochondrial antibody   25       0.050"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abbrev_freq.loc[abbrev_freq.abbrev == \"AMA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T03:06:23.278223Z",
     "start_time": "2021-03-17T03:06:23.276265Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature extraction\n",
    "# if select AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T03:22:47.664058Z",
     "start_time": "2021-03-17T03:22:47.639170Z"
    }
   },
   "outputs": [],
   "source": [
    "def derive_features(abbrev, window_size, replace = True):  # replace = True, replace numbers with zero\n",
    "    samples = data.loc[data.abbrev == abbrev, ]\n",
    "    sample_num = 1\n",
    "    \n",
    "    original_features = pd.DataFrame(columns=[\"id\", \"features\", \"sense\"])\n",
    "    direction_features = pd.DataFrame(columns=[\"id\", \"features\", \"sense\"])\n",
    "    direction_num_features = pd.DataFrame(columns=[\"id\", \"features\", \"sense\"])\n",
    "    \n",
    "    for i in range(samples.shape[0]):  # for each data point\n",
    "        sentence_num = 1\n",
    "        #target_word = samples.iloc[i, 2]  # should avoid using this, as word tokenization would split AC. to AC ., etc.\n",
    "        #target_word = abbrev\n",
    "        \n",
    "        text = samples.iloc[i, 6]\n",
    "        id = i+1  # this is the row number of selected abbreviation\n",
    "        sense = samples.iloc[i, 1]\n",
    "        \n",
    "        target_word = samples.iloc[i, 2]\n",
    "        sentence_length = 0\n",
    "        start_pos = int(samples.iloc[i, 3])\n",
    "        end_pos = int(samples.iloc[i, 4])\n",
    "        \n",
    "        target_word = text[start_pos : end_pos+1]\n",
    "        detect = True\n",
    "        #print(target_word)\n",
    "        # sentence boundary\n",
    "        # one sample can have multiple abbreviations in different sentences.\n",
    "        # before sentence boundary, replace all the numbers with 0 \n",
    "        #print(text)\n",
    "        \n",
    "        \n",
    "        #if replace:\n",
    "        #    text = re.sub(\"\\d+\", \"0\", text)\n",
    "        #    text = re.sub(\"\\d+\\.\\d+\", \"0\", text)\n",
    "\n",
    "        sents = sent_tokenize(text)\n",
    "        exclude = set(string.punctuation)\n",
    "        #s = ''.join(ch for ch in s if ch not in exclude)\n",
    "        for sent in sents:  # for each sentence\n",
    "            #words = [token.lower() for token in word_tokenize(sent)] # word tokenization\n",
    "            # remove punctuations from the words list\n",
    "            sentence_length += (len(sent)+1)\n",
    "            words = [word for word in word_tokenize(sent) if word not in string.punctuation]\n",
    "            \n",
    "#             print(id, \"____________________\")\n",
    "#             print(sent)\n",
    "#             print(words)\n",
    "#             print(target_word)\n",
    "#             print(sentence_length, end_pos)\n",
    "            if sentence_length >= end_pos and (abbrev in words or target_word in words) and detect:\n",
    "                #print(words)\n",
    "                #print(sent)\n",
    "                detect = False\n",
    "                left_features = []\n",
    "                right_features = []\n",
    "                \n",
    "                left_features_direction = []\n",
    "                right_features_direction = []\n",
    "                \n",
    "                left_features_direction_num = []\n",
    "                right_features_direction_num = []\n",
    "                \n",
    "                index = words.index(abbrev)\n",
    "                # find the targeted word\n",
    "                # 1. See if the window-size exceeds the front and back limit\n",
    "                    # If yes, start from the zero-th element, towards right till find the target (features on the left)\n",
    "                            # start from the max-th element, towards left till find the target (features on the right)\n",
    "                    # If no, start from the (index - 5)-th element, towards right till find the target\n",
    "                           # start from the (index - 5)-th element, towards left till find the target\n",
    "                        \n",
    "                # extract features on the left\n",
    "                if index - window_size < 0:\n",
    "                    j = 0\n",
    "                    starting = index\n",
    "                    #while words[j] != target_word:\n",
    "                    while words[j] != abbrev:\n",
    "                        words[j] = ''.join(ch for ch in words[j] if ch not in exclude)\n",
    "                        if replace:\n",
    "                            words[j] = re.sub(\"\\d+\", \"0\", words[j])\n",
    "                            words[j] = re.sub(\"\\d+\\.\\d+\", \"0\", words[j])\n",
    "                        # remove punctuation from the word, meant to fix problem in word tokenization\n",
    "                        # but may cause problem, e.g. p.r.n --> prn\n",
    "                        # may remove later\n",
    "                        \n",
    "                        left_features.append(words[j].lower())\n",
    "                        left_features_direction.append(\"L-\" + words[j].lower())\n",
    "                        left_features_direction_num.append(\"L\" + str(starting) + \"-\" + words[j].lower())\n",
    "                        j += 1\n",
    "                        starting -= 1\n",
    "                else:\n",
    "                    j = index-window_size\n",
    "                    starting = 0\n",
    "                    for k in range(window_size):\n",
    "                        words[j] = ''.join(ch for ch in words[j] if ch not in exclude)\n",
    "                        if replace:\n",
    "                            words[j] = re.sub(\"\\d+\", \"0\", words[j])\n",
    "                            words[j] = re.sub(\"\\d+\\.\\d+\", \"0\", words[j])\n",
    "                        left_features.append(words[j].lower())\n",
    "                        left_features_direction.append(\"L-\" + words[j].lower())\n",
    "                        left_features_direction_num.append(\"L\" + str(window_size-starting) + \"-\" + words[j].lower())\n",
    "                        j += 1\n",
    "                        starting += 1\n",
    "                \n",
    "                # extract feature on the right\n",
    "                if index + window_size >= len(words):\n",
    "                    #j = len(words)-1\n",
    "                    j = index+1\n",
    "                    starting = 1\n",
    "                    while j != len(words):\n",
    "                        words[j] = ''.join(ch for ch in words[j] if ch not in exclude)\n",
    "                        if replace:\n",
    "                            words[j] = re.sub(\"\\d+\", \"0\", words[j])\n",
    "                            words[j] = re.sub(\"\\d+\\.\\d+\", \"0\", words[j])\n",
    "                        right_features.append(words[j].lower())\n",
    "                        right_features_direction.append(\"R-\" + words[j].lower())\n",
    "                        right_features_direction_num.append(\"R\" + str(starting) + \"-\" + words[j].lower())\n",
    "                        j += 1\n",
    "                        starting += 1\n",
    "                else: \n",
    "                    j = index+1\n",
    "                    starting = 1\n",
    "                    for k in range(window_size):\n",
    "                        words[j] = ''.join(ch for ch in words[j] if ch not in exclude)\n",
    "                        if replace:\n",
    "                            words[j] = re.sub(\"\\d+\", \"0\", words[j])\n",
    "                            words[j] = re.sub(\"\\d+\\.\\d+\", \"0\", words[j])\n",
    "                        right_features.append(words[j].lower())\n",
    "                        right_features_direction.append(\"R-\" + words[j].lower())\n",
    "                        right_features_direction_num.append(\"R\" + str(starting) + \"-\" + words[j].lower())\n",
    "                        j += 1\n",
    "                        starting += 1\n",
    "                   \n",
    "                left_features_str = \" \".join(left_features)\n",
    "                right_features_str = \" \".join(right_features)\n",
    "                features = left_features_str + \" \" + right_features_str\n",
    "                \n",
    "#                 print(features)\n",
    "                \n",
    "                left_features_direction_str = \" \".join(left_features_direction)\n",
    "                right_features_direction_str = \" \".join(right_features_direction)\n",
    "                features_direction = left_features_direction_str + \" \" + right_features_direction_str\n",
    "                \n",
    "                left_features_direction_num_str = \" \".join(left_features_direction_num)\n",
    "                right_features_direction_num_str = \" \".join(right_features_direction_num)\n",
    "                features_direction_num = left_features_direction_num_str + \" \" + right_features_direction_num_str\n",
    "                #print(features_direction_num)\n",
    "                \n",
    "#                 if replace:\n",
    "#                     features = re.sub(\"\\d+\", \"0\", features)\n",
    "#                     features = re.sub(\"\\d+\\.\\d+\", \"0\", features)\n",
    "                    \n",
    "#                     features_direction = re.sub(\"\\d+\", \"0\", features_direction)\n",
    "#                     features_direction = re.sub(\"\\d+\\.\\d+\", \"0\", features_direction)\n",
    "                    \n",
    "#                     features_direction_num = features_direction_num+re.sub(\"\\d+\", \"0\", features_direction_num)\n",
    "#                     features_direction_num = features_direction_num+re.sub(\"\\d+\\.\\d+\", \"0\", features_direction_num)\n",
    "#                     print(features_direction_num)\n",
    "                original_features = pd.concat([original_features, pd.DataFrame({\"id\" : [id],\n",
    "                                                                               \"features\" : [features], \n",
    "                                                                               \"sense\" : [sense]})])\n",
    "                \n",
    "                direction_features = pd.concat([direction_features, pd.DataFrame({\"id\" : [id],\n",
    "                                                                               \"features\" : [features_direction], \n",
    "                                                                               \"sense\" : [sense]})])\n",
    "                \n",
    "                direction_num_features = pd.concat([direction_num_features, pd.DataFrame({\"id\" : [id],\n",
    "                                                                               \"features\" : [features_direction_num], \n",
    "                                                                               \"sense\" : [sense]})])\n",
    "            sentence_num += 1\n",
    "        sample_num += 1\n",
    "    return([original_features, direction_features, direction_num_features])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T04:22:58.529067Z",
     "start_time": "2021-03-17T04:22:56.557679Z"
    }
   },
   "outputs": [],
   "source": [
    "ar1, br1, cr1 = derive_features('AMA', 5, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONE-HOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T03:54:27.861035Z",
     "start_time": "2021-03-17T03:54:27.855154Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_features(abbrev, window_size, replace = True):\n",
    "    ar1, br1, cr1 = derive_features(abbrev, window_size, replace = True)\n",
    "    one_hot_vector=[]\n",
    "    for k in cr1['features'].values:\n",
    "        features_direction_num= [item for item in k.split(' ') if item !='']\n",
    "        one_hot_vector=one_hot_vector+features_direction_num\n",
    "    one_hot_vector=sorted(list(set(one_hot_vector)))\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(one_hot_vector))\n",
    "    \n",
    "    onehot_encoded = list()\n",
    "    for k in cr1['features'].values:\n",
    "        features_direction_num= [item for item in k.split(' ') if item !='']\n",
    "        integer_encoded = [char_to_int[char] for char in features_direction_num]\n",
    "        # one hot encode\n",
    "\n",
    "        letter = [0 for _ in range(len(char_to_int))]\n",
    "        for value in integer_encoded:\n",
    "            letter[value] = 1\n",
    "        onehot_encoded.append(letter)\n",
    "    onehot_encoded=np.array(onehot_encoded)\n",
    "    return onehot_encoded,one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "names = {}\n",
    "\n",
    "for i in range(1,6):\n",
    "    features[i],names[i]=one_hot_features('AMA', i, replace=True)#not use ald as they have number of sense=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T04:22:18.630565Z",
     "start_time": "2021-03-17T04:22:08.917017Z"
    }
   },
   "outputs": [],
   "source": [
    "# fea_5,name_5=one_hot_features('AMA', 5, replace=True)#not use ald as they have number of sense=1\n",
    "# fea_4,name_4=one_hot_features('AMA', 4, replace=True)\n",
    "# fea_3,name_3=one_hot_features('AMA', 3, replace=True)\n",
    "# fea_2,name_2=one_hot_features('AMA', 2, replace=True)\n",
    "# fea_1,name_1=one_hot_features('AMA', 1, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# machine learning to decide window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y=cr1['sense'].values\n",
    "\n",
    "train_X = {} \n",
    "test_X = {}\n",
    "train_y = {}\n",
    "test_y = {}\n",
    "\n",
    "for i in range(1,6):\n",
    "    train_X[i],test_X[i],train_y[i],test_y[i] = train_test_split(features[i], y, test_size=0.2, random_state=13, shuffle=True, stratify=y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T04:23:32.515600Z",
     "start_time": "2021-03-17T04:23:32.496231Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_X_5, test_X_5, train_y, test_y = train_test_split(fea_5, y, test_size=0.2, random_state=13, shuffle=True, stratify=y)\n",
    "# train_X_4, test_X_4, train_y, test_y = train_test_split(fea_4, y, test_size=0.2, random_state=13, shuffle=True, stratify=y)\n",
    "# train_X_3, test_X_3, train_y, test_y = train_test_split(fea_3, y, test_size=0.2, random_state=13, shuffle=True, stratify=y)\n",
    "# train_X_2, test_X_2, train_y, test_y = train_test_split(fea_2, y, test_size=0.2, random_state=13, shuffle=True, stratify=y)\n",
    "# train_X_1, test_X_1, train_y, test_y = train_test_split(fea_1, y, test_size=0.2, random_state=13, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T04:30:34.124684Z",
     "start_time": "2021-03-17T04:30:34.119422Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def five_fold_CV(clf, params, dx, dy):\n",
    "    cv_model = RandomizedSearchCV(clf, params, scoring='f1_weighted', n_jobs=-1, \n",
    "                                  cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=13), \n",
    "                                  verbose=1, n_iter=50, refit=True)\n",
    "    \n",
    "    cv_model.fit(dx, dy)\n",
    "    \n",
    "    return cv_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T04:27:30.831646Z",
     "start_time": "2021-03-17T04:27:30.827290Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def logistic_regression_pred(X_train, Y_train, X_test):\n",
    "    lr = LogisticRegression()\n",
    "    tuned_parameters = {'max_iter': [10, 50, 100, 200, 500, 750, 1000], \n",
    "                    'tol': [0.0001, 0.001, 0.01, 0.1],\n",
    "                    'C': [0.01, 0.1, 1.0, 5.0, 10.0, 25.0, 50.0, 100.0],\n",
    "                    'solver': ['lbfgs', 'liblinear', 'newton-cg'], \n",
    "                    'class_weight': [None, 'balanced']}\n",
    "    \n",
    "    best_lr_model = five_fold_CV(lr, tuned_parameters, X_train, Y_train)\n",
    "    lr_y_test = best_lr_model.predict(X_test)\n",
    "    return lr_y_test\n",
    "\n",
    "def svm_pred(X_train, Y_train, X_test):\n",
    "    svm = SVC()\n",
    "    tuned_parameters = {\"C\": [0.001, 0.01, 0.1, 2, 8, 32, 64, 128, 512, 1024, 2048],\n",
    "                    'gamma':['scale', 'auto'],\n",
    "                    'probability':[True], \n",
    "                    'tol': [0.1, 0.01, 0.001, 0.0001]}\n",
    "    best_svm_model = five_fold_CV(svm, tuned_parameters, X_train, Y_train)\n",
    "    svm_y_test = best_svm_model.predict(X_test)\n",
    "    return svm_y_test\n",
    "\n",
    "def knn_pred(X_train, Y_train, X_test):\n",
    "    neigh = KNeighborsClassifier()\n",
    "    tuned_parameters = dict(n_neighbors=range(1, 30), weights=['uniform', 'distance'])\n",
    "    best_knn_model = five_fold_CV(neigh, tuned_parameters, X_train, Y_train)\n",
    "    neigh_y_pred = best_knn_model.predict(X_test)\n",
    "    return neigh_y_pred\n",
    "\n",
    "def randomforest_pred(X_train, Y_train, X_test):\n",
    "    rf = RandomForestClassifier()\n",
    "    \n",
    "    tuned_parameters = {'n_estimators':[50, 100, 250,  500, 750, 1000, 1250, 1500, 2000], \n",
    "                    'criterion':['gini', 'entropy'], \n",
    "                    'max_features':['log2', 'auto', None], \n",
    "                    'min_samples_split':[2, 3, 4], \n",
    "                    'max_depth': [3, 6, 9, 12, 15, 18, 21, 24, 32, None], \n",
    "                    'min_samples_leaf':[1, 2], \n",
    "                    'max_leaf_nodes': [None, 5, 10],\n",
    "                    'min_impurity_decrease':[0.1, 0.01, 0.001, 0.0001, 0.00001],\n",
    "                    'bootstrap': [True, False],\n",
    "                    'class_weight': [None, 'balanced', 'balanced_subsample']}\n",
    "    best_rf_model = five_fold_CV(rf, tuned_parameters, X_train, Y_train)\n",
    "\n",
    "    forest_y_pred = best_rf_model.predict(X_test)\n",
    "    return forest_y_pred \n",
    "\n",
    "def classification_metrics(Y_pred, Y_true):\n",
    "    #TODO: Calculate the above mentioned metrics\n",
    "    #NOTE: It is important to provide the output in the same order\n",
    "    precision = precision_score(Y_pred, Y_true, average='micro')\n",
    "    recall = recall_score(Y_pred, Y_true, average='micro')\n",
    "    f1score = f1_score(Y_pred, Y_true, average='micro')\n",
    "    return precision,recall,f1score\n",
    "\n",
    "def display_metrics(classifierName, Y_pred, Y_true):\n",
    "    print(\"______________________________________________\")\n",
    "    print((\"Classifier: \"+classifierName))\n",
    "    precision, recall, f1score = classification_metrics(Y_pred,Y_true)\n",
    "    print((\"Precision: \"+str(precision)))\n",
    "    print((\"Recall: \"+str(recall)))\n",
    "    print((\"F1-score: \"+str(f1score)))\n",
    "    print(\"______________________________________________\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T04:54:42.621119Z",
     "start_time": "2021-03-17T04:54:42.616950Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import f1_score\n",
    "# from sklearn.metrics import precision_score\n",
    "# from sklearn.metrics import recall_score\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# def report_result(y_test,predicted_target):\n",
    "# #     print('precision',precision_score(y_test, predicted_target,average='macro'))\n",
    "# #     print('f1',f1_score(y_test, predicted_target, average='macro'))\n",
    "# #     print('recall',recall_score(y_test, predicted_target, average='macro'))\n",
    "#     print('recall',accuracy_score(y_test, predicted_target))#recall should be accuracy if in WSD problem\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "______________________________________________\n",
      "Classifier: Logistic Regression\n",
      "Precision: 0.98\n",
      "Recall: 0.98\n",
      "F1-score: 0.98\n",
      "______________________________________________\n",
      "\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "______________________________________________\n",
      "Classifier: Logistic Regression\n",
      "Precision: 0.96\n",
      "Recall: 0.96\n",
      "F1-score: 0.96\n",
      "______________________________________________\n",
      "\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "______________________________________________\n",
      "Classifier: Logistic Regression\n",
      "Precision: 0.96\n",
      "Recall: 0.96\n",
      "F1-score: 0.96\n",
      "______________________________________________\n",
      "\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "______________________________________________\n",
      "Classifier: Logistic Regression\n",
      "Precision: 0.95\n",
      "Recall: 0.95\n",
      "F1-score: 0.9500000000000001\n",
      "______________________________________________\n",
      "\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "______________________________________________\n",
      "Classifier: Logistic Regression\n",
      "Precision: 0.93\n",
      "Recall: 0.93\n",
      "F1-score: 0.93\n",
      "______________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LR - window size (1,5)\n",
    "for i in range(1,6):\n",
    "    display_metrics(\"Logistic Regression\",logistic_regression_pred(train_X[i],train_y[i],test_X[i]),test_y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "______________________________________________\n",
      "Classifier: SVM\n",
      "Precision: 0.91\n",
      "Recall: 0.91\n",
      "F1-score: 0.91\n",
      "______________________________________________\n",
      "\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "______________________________________________\n",
      "Classifier: SVM\n",
      "Precision: 0.95\n",
      "Recall: 0.95\n",
      "F1-score: 0.9500000000000001\n",
      "______________________________________________\n",
      "\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "______________________________________________\n",
      "Classifier: SVM\n",
      "Precision: 0.95\n",
      "Recall: 0.95\n",
      "F1-score: 0.9500000000000001\n",
      "______________________________________________\n",
      "\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "______________________________________________\n",
      "Classifier: SVM\n",
      "Precision: 0.94\n",
      "Recall: 0.94\n",
      "F1-score: 0.94\n",
      "______________________________________________\n",
      "\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "______________________________________________\n",
      "Classifier: SVM\n",
      "Precision: 0.93\n",
      "Recall: 0.93\n",
      "F1-score: 0.93\n",
      "______________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    display_metrics(\"SVM\",svm_pred(train_X[i],train_y[i],test_X[i]),test_y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "______________________________________________\n",
      "Classifier: K Nearest Neighbor\n",
      "Precision: 0.91\n",
      "Recall: 0.91\n",
      "F1-score: 0.91\n",
      "______________________________________________\n",
      "\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "______________________________________________\n",
      "Classifier: K Nearest Neighbor\n",
      "Precision: 0.83\n",
      "Recall: 0.83\n",
      "F1-score: 0.83\n",
      "______________________________________________\n",
      "\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "______________________________________________\n",
      "Classifier: K Nearest Neighbor\n",
      "Precision: 0.66\n",
      "Recall: 0.66\n",
      "F1-score: 0.66\n",
      "______________________________________________\n",
      "\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "______________________________________________\n",
      "Classifier: K Nearest Neighbor\n",
      "Precision: 0.79\n",
      "Recall: 0.79\n",
      "F1-score: 0.79\n",
      "______________________________________________\n",
      "\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "______________________________________________\n",
      "Classifier: K Nearest Neighbor\n",
      "Precision: 0.88\n",
      "Recall: 0.88\n",
      "F1-score: 0.88\n",
      "______________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    display_metrics(\"K Nearest Neighbor\",knn_pred(train_X[i],train_y[i],test_X[i]),test_y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1,6):\n",
    "#     display_metrics(\"RF\",randomforest_pred(train_X[i],train_y[i],test_X[i]),test_y[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
